{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20feb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Model training\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_recall_fscore_support)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435f675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99d7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: CREATING SAMPLE TEXT DATA\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (40, 2)\n",
      "\n",
      "First 5 rows:\n",
      "                                                text  category\n",
      "0  This product is absolutely amazing! Best purch...  positive\n",
      "1      Terrible quality. Waste of money. Do not buy.  negative\n",
      "2       Good value for the price. Works as expected.  positive\n",
      "3  Horrible experience. Product broke after one w...  negative\n",
      "4  Love it! Exceeded my expectations. Highly reco...  positive\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "negative    19\n",
      "positive    18\n",
      "neutral      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Category percentages:\n",
      "category\n",
      "negative    47.5\n",
      "positive    45.0\n",
      "neutral      7.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: CREATE SAMPLE TEXT DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: CREATING SAMPLE TEXT DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sample product reviews dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This product is absolutely amazing! Best purchase ever.\",\n",
    "        \"Terrible quality. Waste of money. Do not buy.\",\n",
    "        \"Good value for the price. Works as expected.\",\n",
    "        \"Horrible experience. Product broke after one week.\",\n",
    "        \"Love it! Exceeded my expectations. Highly recommend.\",\n",
    "        \"Not satisfied. Poor customer service and low quality.\",\n",
    "        \"Decent product. Nothing special but does the job.\",\n",
    "        \"Fantastic! Worth every penny. Will buy again.\",\n",
    "        \"Disappointing. Expected much better for this price.\",\n",
    "        \"Excellent quality and fast shipping. Very happy!\",\n",
    "        \"Complete garbage. Returning it immediately.\",\n",
    "        \"Pretty good. Minor issues but overall satisfied.\",\n",
    "        \"Outstanding! Best in its category. Five stars.\",\n",
    "        \"Mediocre at best. Wouldn't recommend to others.\",\n",
    "        \"Incredible product. Changed my life for the better.\",\n",
    "        \"Worst purchase ever. Total waste of time and money.\",\n",
    "        \"Satisfied with my purchase. Good quality product.\",\n",
    "        \"Awful. Cheaply made and doesn't work properly.\",\n",
    "        \"Great buy! Exactly what I needed. Thank you!\",\n",
    "        \"Regret buying this. Save your money.\",\n",
    "        \"Amazing quality and design. Love everything about it!\",\n",
    "        \"Not worth it. Better alternatives available elsewhere.\",\n",
    "        \"Perfect! No complaints whatsoever. Highly satisfied.\",\n",
    "        \"Broken on arrival. Very disappointed with this.\",\n",
    "        \"Superb product. Elegant design and great functionality.\",\n",
    "        \"Barely works. Customer support was unhelpful too.\",\n",
    "        \"Really happy with this purchase. Good investment.\",\n",
    "        \"Defective item received. Poor quality control.\",\n",
    "        \"Brilliant! Solves all my problems. Couldn't be happier.\",\n",
    "        \"Absolute trash. Would give zero stars if possible.\",\n",
    "        \"Nice product overall. Some room for improvement.\",\n",
    "        \"Nightmare experience. Never ordering from here again.\",\n",
    "        \"Wonderful quality. Beautifully packaged and delivered.\",\n",
    "        \"Junk. Falls apart easily. Very frustrating.\",\n",
    "        \"Impressive! Better than advertised. Great deal.\",\n",
    "        \"Unacceptable quality. Demanding a full refund.\",\n",
    "        \"Solid purchase. Reliable and durable product.\",\n",
    "        \"Pathetic. Doesn't match the description at all.\",\n",
    "        \"Delighted with this! Perfect for my needs.\",\n",
    "        \"Horrible material. Feels cheap and flimsy.\"\n",
    "    ],\n",
    "    'category': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'negative', 'neutral', 'positive', 'negative', 'positive',\n",
    "        'negative', 'neutral', 'positive', 'negative', 'positive',\n",
    "        'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "        'neutral', 'negative', 'positive', 'negative', 'positive',\n",
    "        'negative', 'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nCategory percentages:\")\n",
    "print(df['category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bbc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Text Statistics:\n",
      "       text_length  word_count\n",
      "count    40.000000   40.000000\n",
      "mean     48.250000    7.025000\n",
      "std       4.259228    0.831665\n",
      "min      36.000000    5.000000\n",
      "25%      45.750000    6.750000\n",
      "50%      48.000000    7.000000\n",
      "75%      51.250000    8.000000\n",
      "max      55.000000    9.000000\n",
      "\n",
      "Average text length by category:\n",
      "category\n",
      "negative    47.315789\n",
      "neutral     48.333333\n",
      "positive    49.222222\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Average word count by category:\n",
      "category\n",
      "negative    6.894737\n",
      "neutral     7.333333\n",
      "positive    7.111111\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "text           0\n",
      "category       0\n",
      "text_length    0\n",
      "word_count     0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nText Statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())\n",
    "\n",
    "print(\"\\nAverage text length by category:\")\n",
    "print(df.groupby('category')['text_length'].mean())\n",
    "\n",
    "print(\"\\nAverage word count by category:\")\n",
    "print(df.groupby('category')['word_count'].mean())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9216b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: TEXT PREPROCESSING - CLEANING\n",
      "================================================================================\n",
      "\n",
      "Example of text cleaning:\n",
      "Original: This product is absolutely amazing! Best purchase ever.\n",
      "Cleaned:  this product is absolutely amazing best purchase ever\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: TEXT PREPROCESSING - CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: TEXT PREPROCESSING - CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text data\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\nExample of text cleaning:\")\n",
    "print(f\"Original: {df['text'].iloc[0]}\")\n",
    "print(f\"Cleaned:  {df['cleaned_text'].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5fd2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: TEXT PREPROCESSING - TOKENIZATION\n",
      "================================================================================\n",
      "\n",
      "Example of tokenization:\n",
      "Text: this product is absolutely amazing best purchase ever\n",
      "Tokens: ['this', 'product', 'is', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: TEXT PREPROCESSING - TOKENIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: TEXT PREPROCESSING - TOKENIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "\n",
    "print(\"\\nExample of tokenization:\")\n",
    "print(f\"Text: {df['cleaned_text'].iloc[0]}\")\n",
    "print(f\"Tokens: {df['tokens'].iloc[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0408ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: TEXT PREPROCESSING - REMOVE STOPWORDS\n",
      "================================================================================\n",
      "\n",
      "Number of stopwords: 198\n",
      "Sample stopwords: ['before', 'through', 'isn', 'me', 'ma', \"should've\", 'their', \"you'd\", 'off', \"shouldn't\"]\n",
      "\n",
      "Example of stopword removal:\n",
      "Before: ['this', 'product', 'is', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n",
      "After:  ['product', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: TEXT PREPROCESSING - REMOVE STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: TEXT PREPROCESSING - REMOVE STOPWORDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"\\nNumber of stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from tokens\"\"\"\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply stopword removal\n",
    "df['tokens_no_stop'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\nExample of stopword removal:\")\n",
    "print(f\"Before: {df['tokens'].iloc[0]}\")\n",
    "print(f\"After:  {df['tokens_no_stop'].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e57ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: TEXT PREPROCESSING - LEMMATIZATION\n",
      "================================================================================\n",
      "\n",
      "Example of lemmatization:\n",
      "Before: ['product', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n",
      "After:  ['product', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n",
      "\n",
      "Comparison - Lemmatization vs Stemming:\n",
      "Lemmatized: ['product', 'absolutely', 'amazing', 'best', 'purchase', 'ever']\n",
      "Stemmed:    ['product', 'absolut', 'amaz', 'best', 'purchas', 'ever']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: TEXT PREPROCESSING - LEMMATIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: TEXT PREPROCESSING - LEMMATIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens to their base form\"\"\"\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization\n",
    "df['tokens_lemmatized'] = df['tokens_no_stop'].apply(lemmatize_tokens)\n",
    "\n",
    "print(\"\\nExample of lemmatization:\")\n",
    "print(f\"Before: {df['tokens_no_stop'].iloc[0]}\")\n",
    "print(f\"After:  {df['tokens_lemmatized'].iloc[0]}\")\n",
    "\n",
    "# Alternative: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Stem tokens\"\"\"\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "\n",
    "df['tokens_stemmed'] = df['tokens_no_stop'].apply(stem_tokens)\n",
    "\n",
    "print(\"\\nComparison - Lemmatization vs Stemming:\")\n",
    "print(f\"Lemmatized: {df['tokens_lemmatized'].iloc[0]}\")\n",
    "print(f\"Stemmed:    {df['tokens_stemmed'].iloc[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06168a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: FINAL PREPROCESSED TEXT\n",
      "================================================================================\n",
      "\n",
      "Preprocessing pipeline complete!\n",
      "\n",
      "Example comparison:\n",
      "Original:   This product is absolutely amazing! Best purchase ever.\n",
      "Processed:  product absolutely amazing best purchase ever\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 7: CONVERT BACK TO TEXT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: FINAL PREPROCESSED TEXT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Join tokens back to text\n",
    "df['processed_text'] = df['tokens_lemmatized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\"\\nPreprocessing pipeline complete!\")\n",
    "print(\"\\nExample comparison:\")\n",
    "print(f\"Original:   {df['text'].iloc[0]}\")\n",
    "print(f\"Processed:  {df['processed_text'].iloc[0]}\")\n",
    "\n",
    "# ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37fa6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: FEATURE EXTRACTION - TF-IDF\n",
      "================================================================================\n",
      "\n",
      "TF-IDF Feature Matrix Shape: (40, 100)\n",
      "Number of features: 100\n",
      "\n",
      "Top 20 features:\n",
      "['absolute' 'amazing' 'best' 'better' 'buy' 'customer' 'design' 'doesnt'\n",
      " 'ever' 'expected' 'experience' 'good' 'great' 'happy' 'highly' 'horrible'\n",
      " 'love' 'match' 'match description' 'material']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: FEATURE EXTRACTION - TF-IDF\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: FEATURE EXTRACTION - TF-IDF\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data\n",
    "X = df['processed_text']\n",
    "y = df['category']\n",
    "\n",
    "# Method 1: TF-IDF Vectorizer (recommended)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Top 100 features\n",
    "    min_df=1,          # Minimum document frequency\n",
    "    max_df=0.8,        # Maximum document frequency\n",
    "    ngram_range=(1, 2) # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"\\nTF-IDF Feature Matrix Shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nTop 20 features:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27fa615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count Vectorizer Shape: (40, 100)\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Count Vectorizer (Bag of Words)\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"\\nCount Vectorizer Shape: {X_count.shape}\")\n",
    "\n",
    "# We'll use TF-IDF for training\n",
    "X_features = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6ae460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "Training set size: 32 samples\n",
      "Testing set size: 8 samples\n",
      "\n",
      "Training set category distribution:\n",
      "category\n",
      "positive    15\n",
      "negative    15\n",
      "neutral      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTraining set category distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d2c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
